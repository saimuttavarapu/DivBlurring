{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5630fc53",
   "metadata": {},
   "source": [
    "# Training the BivBlurring model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674becbc",
   "metadata": {},
   "source": [
    "In this notebook we are training the DivBlurring model. For the training, we are using the PyTorch_Lighiting framefwork on synthatic data. This data generated based on the realistic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f31e450",
   "metadata": {},
   "source": [
    "## Import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db96a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "from torch.distributions import normal\n",
    "import matplotlib.pyplot as plt, numpy as np, pickle\n",
    "from scipy.stats import norm\n",
    "from tifffile import imread\n",
    "import sys\n",
    "from sklearn.feature_extraction import image\n",
    "from tqdm import tqdm\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05894b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from glob import glob\n",
    "from tifffile import imsave\n",
    "from sklearn.cluster import MeanShift\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from torch.nn import init\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b869c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f1ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "from torch.nn import init\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645bcf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda:0\") \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c10f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities\n",
    "import loss_function\n",
    "import network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f0f87",
   "metadata": {},
   "source": [
    "## Download and load the DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the data is avialable in the data folder.\n",
    "\n",
    "datai = []\n",
    "observation= []\n",
    "for i in range(1): # As data in multiple files, looping over the all files.\n",
    "    datai= imread(\"./data/data4Sai/\"+'BluryNoisy_tubulins_'+str(i+1)+'_SOFImodel.tif')\n",
    "    observation.extend(datai)\n",
    "observation = np.array(observation)\n",
    "\n",
    "print(observation.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b806d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data and shape of the total data set.\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.title(label='Single raw Image')\n",
    "plt.imshow(observation[0],cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print(\"Total number imgs in the give dataset:\" + str(observation.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6252104",
   "metadata": {},
   "source": [
    "Spli the data 85% for the thraining and 15% for validation. We chose maximum for training as our synthatic data containg 7000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4133f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images = utilities.get_split_data(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab427739",
   "metadata": {},
   "source": [
    "## Convert data to tensor for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd902d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor, x_val_tensor, data_mean, data_std = utilities.preprocess(train_images, val_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ccf46",
   "metadata": {},
   "source": [
    "## Train the DivBlurring model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad111ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_depth=2\n",
    "batch_size=32\n",
    "max_epochs=150\n",
    "model_name = 'models_DivBlurring_PCReg_1e3' # a name used to identify the model\n",
    "basedir = 'models_DivBlurring_PCReg_1e3' # the base directory in which our model will live\n",
    "real_noise=False\n",
    "noise_model = None\n",
    "gaussian_noise_std = 10\n",
    "reg_parameter = 1e-3\n",
    "sigma = 3\n",
    "shape = 256\n",
    "img_per_each_epochs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(x_train_tensor,x_val_tensor,batch_size):\n",
    "    \"\"\"Conver the data into dataloaders.\n",
    "    \"\"\"\n",
    "    train_dataset = MyDataset(x_train_tensor,x_train_tensor)\n",
    "    val_dataset = MyDataset(x_val_tensor,x_val_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader,val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e3d9e",
   "metadata": {},
   "source": [
    "## Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df460cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(x_train_tensor, x_val_tensor, batch_size, data_mean, data_std, gaussian_noise_std, \n",
    "                  noise_model,n_depth, max_epochs, model_name, basedir, log_info=False):\n",
    "    \n",
    "    train_loader,val_loader = create_dataloaders(x_train_tensor, x_val_tensor, batch_size)\n",
    "    collapse_flag = True\n",
    "    if not os.path.exists(basedir):\n",
    "        os.makedirs(basedir)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath=basedir,\n",
    "    filename=model_name+'_best',\n",
    "    save_last=True,\n",
    "    save_top_k=1,\n",
    "    mode='min',)\n",
    "    checkpoint_callback.CHECKPOINT_NAME_LAST = model_name+\"_last\"\n",
    "    logger = TensorBoardLogger(basedir, name= \"\", version=\"\", default_hp_metric=False)\n",
    "    weights_summary=\"full\" #if log_info else None\n",
    "    if not log_info:\n",
    "        pl.utilities.distributed.log.setLevel(logging.ERROR)\n",
    "    posterior_collapse_count = 0\n",
    "    \n",
    "    while collapse_flag and posterior_collapse_count<20:\n",
    "#         print(\"create vae model\")\n",
    "        collapse_flag, vae = utilities.create_model_and_train(basedir,data_mean,data_std,gaussian_noise_std,noise_model,\n",
    "                                               n_depth,max_epochs,logger,checkpoint_callback,\n",
    "                                               train_loader,val_loader,kl_annealing=False, weights_summary=weights_summary)\n",
    "        if collapse_flag:\n",
    "            posterior_collapse_count=posterior_collapse_count+1\n",
    "        \n",
    "    if collapse_flag:\n",
    "        print(\"Posterior collapse limit reached, attempting training with KL annealing turned on!\")\n",
    "        while collapse_flag:\n",
    "            collapse_flag, vae = utilities.create_model_and_train(basedir,data_mean,data_std,gaussian_noise_std,noise_model,\n",
    "                                               n_depth,max_epochs,logger,checkpoint_callback,\n",
    "                                               train_loader,val_loader,kl_annealing=True, weights_summary=weights_summary)\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = train_network(x_train_tensor, x_val_tensor, batch_size, data_mean, data_std, \n",
    "                       gaussian_noise_std, noise_model, n_depth=n_depth, max_epochs=max_epochs, \n",
    "                       model_name=model_name, basedir=basedir, log_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c303a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('updDivNoising': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a48566917f2d7885bc685087ba06e234e51f695c26e8557391e1a3095fa44fa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
